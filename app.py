# -*- coding: utf-8 -*-
# SEO Crawler Audit (Streamlit, Cloud-ready)
# æ©Ÿèƒ½: robots.txtæº–æ‹ ã§ã‚µã‚¤ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€ä¸»è¦ãªSEOä¸å‚™ã‚’è‡ªå‹•æŠ½å‡ºãƒ»CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

import asyncio, re, time, io, csv
from dataclasses import dataclass, asdict
from urllib.parse import urljoin, urldefrag, urlparse
import urllib.robotparser as robotparser

import aiohttp
from aiohttp import ClientTimeout
import streamlit as st
from bs4 import BeautifulSoup
import tldextract

# =============== UI ===============
st.set_page_config(page_title="SEO Crawler Audit", layout="wide")
st.title("ğŸ•·ï¸ SEO Crawler Auditï¼ˆWebã‚¢ãƒ—ãƒªï¼‰")
st.write("URLã‚’å…¥åŠ›ã—ã¦ï¼»ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹ï¼½ã‚’æŠ¼ã™ã¨ã€ã‚µã‚¤ãƒˆå†…ã®ä¸»è¦ãªSEOä¸å‚™ã‚’è‡ªå‹•æŠ½å‡ºã—ã¾ã™ã€‚")

col = st.columns(4)
start_url = col[0].text_input("é–‹å§‹URLï¼ˆä¾‹: https://example.com/ï¼‰", "https://example.com/")
max_pages = col[1].number_input("æœ€å¤§ã‚¯ãƒ­ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸æ•°", 10, 5000, 200, step=10)
max_depth = col[2].number_input("æœ€å¤§æ·±ã•", 1, 20, 5)
concurrency = col[3].number_input("åŒæ™‚æ¥ç¶šæ•°", 1, 32, 8)

col2 = st.columns(4)
delay_ms = col2[0].number_input("ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆms/ãƒ›ã‚¹ãƒˆï¼‰", 0, 5000, 200)
ua = col2[1].text_input("User-Agent", "SEO-Audit-Bot/1.0 (+https://example.com)")
same_registrable = col2[2].selectbox("ã‚¯ãƒ­ãƒ¼ãƒ«ç¯„å›²", ["åŒä¸€ãƒ›ã‚¹ãƒˆã®ã¿", "åŒä¸€ãƒ¬ã‚¸ã‚¹ãƒˆãƒ©ãƒ–ãƒ«ãƒ‰ãƒ¡ã‚¤ãƒ³"], index=1)
respect_robots = col2[3].checkbox("robots.txtã‚’å°Šé‡", value=True)

inc_pat = st.text_input("å«ã‚ã‚‹URLãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ­£è¦è¡¨ç¾ã€ä»»æ„ï¼‰", "")
exc_pat = st.text_input("é™¤å¤–ã™ã‚‹URLãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ­£è¦è¡¨ç¾ã€ä»»æ„ï¼‰", r"\.(pdf|jpg|jpeg|png|gif|svg|webp|css|js|zip|mp4|mp3)(\?|$)")

run = st.button("ğŸš€ ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹")

# =============== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ===============
def norm_url(u: str, base: str) -> str:
    if not u: return ""
    u = urljoin(base, u)
    u, _ = urldefrag(u)  # remove fragment
    return u

def same_scope(u: str, seed: str, registrable: bool) -> bool:
    pu, ps = urlparse(u), urlparse(seed)
    if registrable:
        du = tldextract.extract(pu.netloc)
        ds = tldextract.extract(ps.netloc)
        return (du.domain, du.suffix) == (ds.domain, ds.suffix)
    return pu.netloc == ps.netloc

def title_len_ok(t):
    if not t: return False, "ã‚¿ã‚¤ãƒˆãƒ«æ¬ è½"
    l = len(t.strip())
    if l < 30: return False, f"ã‚¿ã‚¤ãƒˆãƒ«çŸ­ã„({l})"
    if l > 65: return False, f"ã‚¿ã‚¤ãƒˆãƒ«é•·ã„({l})"
    return True, ""

def desc_len_ok(d):
    if not d: return False, "ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³æ¬ è½"
    l = len(d.strip())
    if l < 70: return False, f"DçŸ­ã„({l})"
    if l > 160: return False, f"Dé•·ã„({l})"
    return True, ""

def words_count(text: str) -> int:
    return len(re.findall(r"\w+", text or ""))

def is_html(resp) -> bool:
    ct = resp.headers.get("Content-Type","").lower()
    return "text/html" in ct or "application/xhtml+xml" in ct

def parse_robots(seed: str, ua: str):
    rp = robotparser.RobotFileParser()
    origin = f"{urlparse(seed).scheme}://{urlparse(seed).netloc}"
    rp.set_url(urljoin(origin, "/robots.txt"))
    try:
        rp.read()
    except Exception:
        pass
    return rp

def xrobots_noindex(headers) -> bool:
    v = headers.get("x-robots-tag", "")
    return "noindex" in v.lower()

def xrobots_nofollow(headers) -> bool:
    v = headers.get("x-robots-tag", "")
    return "nofollow" in v.lower()

# =============== ãƒ‡ãƒ¼ã‚¿æ§‹é€  ===============
@dataclass
class PageAudit:
    url: str
    status: int
    depth: int
    final_url: str
    redirected: int
    canonical: str
    canonical_status: str
    robots_meta: str
    noindex: bool
    nofollow: bool
    x_noindex: bool
    x_nofollow: bool
    title: str
    title_issue: str
    description: str
    desc_issue: str
    h1_count: int
    images: int
    images_missing_alt: int
    internal_links: int
    external_links: int
    broken_internal_links: int
    word_count: int

# =============== ã‚³ã‚¢ã‚¯ãƒ­ãƒ¼ãƒ« ===============
async def crawl(seed, max_pages, max_depth, concurrency, delay_ms, ua, inc_pat, exc_pat, respect_robots, registrable_scope):
    sem = asyncio.Semaphore(concurrency)
    seen, results, link_graph = set(), [], {}
    queue = asyncio.Queue()
    await queue.put((seed, 0))
    seen.add(seed)

    include_re = re.compile(inc_pat) if inc_pat else None
    exclude_re = re.compile(exc_pat) if exc_pat else None
    rp = parse_robots(seed, ua) if respect_robots else None
    last_req_time = {}
    TIMEOUT = ClientTimeout(total=20)

    async def polite_wait(host):
        # ç°¡æ˜“ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡ï¼šãƒ›ã‚¹ãƒˆã”ã¨ã«delay_mså¾…æ©Ÿ
        if delay_ms <= 0: return
        t = time.time()
        last = last_req_time.get(host, 0)
        wait = (last + delay_ms/1000.0) - t
        if wait > 0:
            await asyncio.sleep(wait)

    async with aiohttp.ClientSession(timeout=TIMEOUT, headers={"User-Agent": ua}) as session:
        async def fetch(url, depth):
            host = urlparse(url).netloc
            await polite_wait(host)
            last_req_time[host] = time.time()
            redirected = 0
            try:
                async with session.get(url, allow_redirects=True) as resp:
                    status = resp.status
                    final_url = str(resp.url)
                    redirected = len(resp.history)
                    if not is_html(resp):
                        return status, final_url, "", None
                    html = await resp.text(errors="ignore")
                    return status, final_url, html, resp.headers
            except Exception:
                return 0, url, "", {}

        async def worker():
            nonlocal results
            while not queue.empty() and len(results) < max_pages:
                url, depth = await queue.get()

                # robots.txt
                if respect_robots and rp is not None and not rp.can_fetch(ua, url):
                    queue.task_done()
                    continue

                status, final_url, html, headers = await fetch(url, depth)
                if not html:
                    # éHTML or ã‚¨ãƒ©ãƒ¼
                    results.append(PageAudit(
                        url=url, status=status, depth=depth, final_url=final_url, redirected=0,
                        canonical="", canonical_status="", robots_meta="", noindex=False, nofollow=False,
                        x_noindex=xrobots_noindex(headers or {}), x_nofollow=xrobots_nofollow(headers or {}),
                        title="", title_issue="éHTML/å–å¾—ä¸å¯", description="", desc_issue="",
                        h1_count=0, images=0, images_missing_alt=0, internal_links=0, external_links=0,
                        broken_internal_links=0, word_count=0
                    ))
                    queue.task_done()
                    continue

                soup = BeautifulSoup(html, "html.parser")

                # robots meta
                robots_meta = ""
                meta_robots = soup.find("meta", attrs={"name":"robots"})
                if meta_robots and meta_robots.get("content"):
                    robots_meta = meta_robots.get("content","").lower()
                noindex = "noindex" in robots_meta
                nofollow = "nofollow" in robots_meta

                # title / description
                title = (soup.title.string.strip() if soup.title and soup.title.string else "").strip()
                title_ok, title_issue = title_len_ok(title)
                desc = ""
                md = soup.find("meta", attrs={"name":"description"})
                if md and md.get("content"):
                    desc = md.get("content","").strip()
                desc_ok, desc_issue = desc_len_ok(desc)

                # canonical
                canonical = ""
                link_c = soup.find("link", rel=lambda v: v and "canonical" in v)
                if link_c and link_c.get("href"):
                    canonical = norm_url(link_c.get("href"), final_url)
                canonical_status = "OK"
                if canonical and canonical.rstrip("/") != final_url.rstrip("/"):
                    canonical_status = "è‡ªå·±å‚ç…§ã§ã¯ãªã„"

                # hreflangï¼ˆå­˜åœ¨ãƒã‚§ãƒƒã‚¯ã®ã¿ï¼‰
                # hreflangs = soup.find_all("link", rel=lambda v: v and "alternate" in v, hreflang=True)

                # images alt
                images = soup.find_all("img")
                img_total = len(images)
                img_miss = sum(1 for im in images if not im.get("alt"))

                # links
                a_tags = soup.find_all("a", href=True)
                intern = extern = 0
                broken_internal = 0
                children = []
                for a in a_tags:
                    href = norm_url(a.get("href"), final_url)
                    if not href.startswith(("http://","https://")):
                        continue
                    # ãƒ•ã‚£ãƒ«ã‚¿
                    if include_re and not include_re.search(href):
                        continue
                    if exclude_re and exclude_re.search(href):
                        continue
                    if not same_scope(href, start_url, registrable_scope):
                        extern += 1
                        continue
                    intern += 1
                    children.append(href)

                link_graph[final_url] = children

                # ãƒ¯ãƒ¼ãƒ‰æ•°ï¼ˆæœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã®æ¦‚ç®—ï¼‰
                for s in soup(["script","style","noscript"]): s.decompose()
                text = soup.get_text(" ", strip=True)
                wc = words_count(text)

                # ãƒšãƒ¼ã‚¸çµæœ
                results.append(PageAudit(
                    url=url, status=status, depth=depth, final_url=final_url, redirected=redirected,
                    canonical=canonical, canonical_status=canonical_status,
                    robots_meta=robots_meta, noindex=noindex, nofollow=nofollow,
                    x_noindex=xrobots_noindex(headers or {}), x_nofollow=xrobots_nofollow(headers or {}),
                    title=title, title_issue=("" if title_ok else title_issue),
                    description=desc, desc_issue=("" if desc_ok else desc_issue),
                    h1_count=len(soup.find_all("h1")),
                    images=img_total, images_missing_alt=img_miss,
                    internal_links=intern, external_links=extern,
                    broken_internal_links=broken_internal,  # ç°¡æ˜“
                    word_count=wc
                ))

                # æ¬¡URLæŠ•å…¥ï¼ˆBFSï¼‰
                if depth < max_depth:
                    for nxt in children:
                        if nxt in seen: continue
                        if len(results) + queue.qsize() >= max_pages: break
                        if respect_robots and rp is not None and not rp.can_fetch(ua, nxt):
                            continue
                        seen.add(nxt)
                        await queue.put((nxt, depth+1))

                queue.task_done()

        workers = [asyncio.create_task(worker()) for _ in range(concurrency)]
        await queue.join()
        for w in workers:
            w.cancel()
        return results

# =============== å®Ÿè¡Œ ===============
if run:
    # å…¥åŠ›ãƒã‚§ãƒƒã‚¯
    try:
        p = urlparse(start_url)
        assert p.scheme in ("http","https") and p.netloc
    except Exception:
        st.error("é–‹å§‹URLãŒä¸æ­£ã§ã™ã€‚https:// ã‹ã‚‰å§‹ã¾ã‚‹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    st.info("ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚å®Œäº†ã¾ã§ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚")
    progress = st.progress(0)
    status_box = st.empty()

    # é€²æ—ã®è¦‹ã›æ–¹ï¼šé©å½“ã«æ•°å›æ›´æ–°ï¼ˆç°¡æ˜“ï¼‰
    async def run_crawl():
        registrable_scope = (same_registrable == "åŒä¸€ãƒ¬ã‚¸ã‚¹ãƒˆãƒ©ãƒ–ãƒ«ãƒ‰ãƒ¡ã‚¤ãƒ³")
        res = await crawl(
            start_url.strip(),
            int(max_pages),
            int(max_depth),
            int(concurrency),
            int(delay_ms),
            ua.strip(),
            inc_pat.strip(),
            exc_pat.strip(),
            respect_robots,
            registrable_scope
        )
        return res

    results = asyncio.run(run_crawl())

    # é€²æ—UIæ›´æ–°ï¼ˆå®Œäº†ï¼‰
    progress.progress(100)
    status_box.success(f"ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†: {len(results)}ãƒšãƒ¼ã‚¸")

    # é›†è¨ˆã¨ä¸å‚™æŠ½å‡º
    import pandas as pd
    df = pd.DataFrame([asdict(r) for r in results])

    # é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«ã€è–„ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ãƒ¡ã‚¿æ¬ è½ ç­‰
    issues = []
    if not df.empty:
        # é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«
        dup_titles = (df[df["title"].str.len()>0]
                      .groupby("title").size().reset_index(name="count")
                      .query("count > 1"))
        if not dup_titles.empty:
            issues.append(f"é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ« {len(dup_titles)}ä»¶")

        # ã‚¿ã‚¤ãƒˆãƒ«/ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³æ¬ è½ãƒ»é•·çŸ­
        bad_title = df[df["title_issue"]!=""]
        bad_desc  = df[df["desc_issue"]!=""]

        # noindex/nofollow
        noindex_pages = df[df["noindex"] | df["x_noindex"]]
        # H1ç•°å¸¸
        h1_anom = df[(df["h1_count"]==0) | (df["h1_count"]>1)]
        # ç”»åƒaltæ¬ è½ç‡
        img_rows = df[df["images"]>0]
        img_bad = img_rows[ img_rows["images_missing_alt"]/img_rows["images"] > 0.3 ]
        # è–„ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        thin = df[df["word_count"] < 300]

        summary = {
            "ã‚¯ãƒ­ãƒ¼ãƒ«ç·æ•°": len(df),
            "200ãƒšãƒ¼ã‚¸æ•°": int((df["status"]==200).sum()),
            "ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ": int((df["redirected"]>0).sum()),
            "ã‚¨ãƒ©ãƒ¼(>=400)": int((df["status"]>=400).sum()),
            "ã‚¿ã‚¤ãƒˆãƒ«å•é¡Œ": len(bad_title),
            "ãƒ¡ã‚¿Då•é¡Œ": len(bad_desc),
            "noindexæ¤œå‡º": len(noindex_pages),
            "H1ç•°å¸¸": len(h1_anom),
            "ç”»åƒalt>30%æ¬ è½": len(img_bad),
            "è–„ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„(<300èª)": len(thin),
            "é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«ã‚°ãƒ«ãƒ¼ãƒ—": len(dup_titles),
        }

        st.subheader("ğŸ“Š ã‚µãƒãƒªãƒ¼")
        st.table(pd.DataFrame(list(summary.items()), columns=["é …ç›®","ä»¶æ•°"]))

        st.subheader("ğŸ› ï¸ ä¸å‚™ãƒªã‚¹ãƒˆï¼ˆä¸»è¦ï¼‰")
        tabs = st.tabs(["ã‚¿ã‚¤ãƒˆãƒ«å•é¡Œ","ãƒ¡ã‚¿Då•é¡Œ","noindex","H1ç•°å¸¸","ç”»åƒaltæ¬ è½ç‡é«˜","è–„ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„","é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«"])
        with tabs[0]:
            st.dataframe(bad_title[["final_url","title","title_issue","status","depth"]])
        with tabs[1]:
            st.dataframe(bad_desc[["final_url","description","desc_issue","status","depth"]])
        with tabs[2]:
            st.dataframe(noindex_pages[["final_url","robots_meta","x_noindex","x_nofollow","status"]])
        with tabs[3]:
            st.dataframe(h1_anom[["final_url","h1_count","status","depth","title"]])
        with tabs[4]:
            st.dataframe(img_bad[["final_url","images","images_missing_alt","status","depth"]])
        with tabs[5]:
            st.dataframe(thin[["final_url","word_count","status","depth","title"]])
        with tabs[6]:
            if dup_titles.empty:
                st.write("é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«ãªã—")
            else:
                st.dataframe(dup_titles)

        st.subheader("ğŸ“„ å…¨ãƒšãƒ¼ã‚¸çµæœ")
        st.dataframe(df)

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆCSVï¼‰
        csv_buf = io.StringIO()
        df.to_csv(csv_buf, index=False, encoding="utf-8")
        st.download_button("ğŸ“¥ å…¨çµæœCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_buf.getvalue().encode("utf-8"),
                           file_name="seo_crawl_audit.csv", mime="text/csv")

        # ãƒ¬ãƒãƒ¼ãƒˆï¼ˆç°¡æ˜“HTMLï¼‰
        html_buf = io.StringIO()
        html_buf.write("<html><head><meta charset='utf-8'><title>SEO Crawl Report</title></head><body>")
        html_buf.write("<h1>SEO Crawl Report</h1>")
        html_buf.write("<h2>Summary</h2><ul>")
        for k,v in summary.items():
            html_buf.write(f"<li>{k}: {v}</li>")
        html_buf.write("</ul>")
        html_buf.write("<h2>Pages</h2>")
        html_buf.write(df.to_html(index=False))
        html_buf.write("</body></html>")
        st.download_button("ğŸ“¥ HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=html_buf.getvalue().encode("utf-8"),
                           file_name="seo_crawl_report.html", mime="text/html")

    else:
        st.warning("æœ‰åŠ¹ãªãƒšãƒ¼ã‚¸ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚é–‹å§‹URL/robots/ç¯„å›²è¨­å®šã‚’ã”ç¢ºèªãã ã•ã„ã€‚")

