# app.py â€” crawl -> metrics -> LLM suggestions -> dashboard (secure edition)
import asyncio, json, io, csv
import streamlit as st
from secure_openai_client import get_openai_client
from crawler import crawl_site
from analyzer import compute_metrics
from llm import page_audit

st.set_page_config(page_title="Site Crawl & Audit (Safe)", page_icon="ğŸ•¸ï¸")
st.title("ã‚µã‚¤ãƒˆè‡ªå‹•ã‚¯ãƒ­ãƒ¼ãƒ« Ã— ChatGPTåˆ†æï¼ˆå®‰å…¨å®Ÿè£…ãƒ»æ‹¡å¼µç‰ˆï¼‰")

# èµ·å‹•æ™‚ï¼šOpenAIç–é€šï¼ˆè»½é‡ï¼‰
client = get_openai_client()
try:
    _ = client.models.list()
    st.caption("ğŸŸ¢ OpenAI: æ¥ç¶šç¢ºèªOK")
except Exception as e:
    st.error(f"OpenAIæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e.__class__.__name__}")
    st.stop()

root_url = st.text_input("é–‹å§‹URLï¼ˆåŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã‚’å¯¾è±¡ï¼‰", placeholder="https://example.com/")
max_pages = st.slider("æœ€å¤§ã‚¯ãƒ­ãƒ¼ãƒ«æ•°", 5, 300, 40)

if "cancel" not in st.session_state:
    st.session_state.cancel = False
if "running" not in st.session_state:
    st.session_state.running = False

col1, col2 = st.columns(2)
start_btn = col1.button("ã‚¯ãƒ­ãƒ¼ãƒ« + åˆ†æ é–‹å§‹", disabled=st.session_state.running)
cancel_btn = col2.button("ä¸­æ–­", disabled=not st.session_state.running)

if cancel_btn:
    st.session_state.cancel = True
    st.info("ä¸­æ–­ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸâ€¦")

def run_async(coro):
    try:
        return asyncio.run(coro)
    except RuntimeError:
        loop = asyncio.new_event_loop()
        try:
            asyncio.set_event_loop(loop)
            return loop.run_until_complete(coro)
        finally:
            try:
                loop.run_until_complete(loop.shutdown_asyncgens())
            except Exception:
                pass
            loop.close()

if start_btn:
    if not root_url.strip():
        st.warning("é–‹å§‹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    st.session_state.running = True
    st.session_state.cancel = False

    progress = st.empty()
    status_box = st.empty()

    async def main():
        progress.progress(0.0, text="ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­â€¦")
        try:
            pages = await crawl_site(root_url.strip(), max_pages=max_pages)
        except Exception as e:
            st.error(f"ã‚¯ãƒ­ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {e.__class__.__name__}")
            return {}, {}

        if st.session_state.cancel or not pages:
            return {}, {}

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        progress.progress(0.4, text="ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç®—å‡ºä¸­â€¦")
        metrics_map = {u: compute_metrics(p) for u, p in pages.items()}

        # LLMææ¡ˆ
        progress.progress(0.7, text="LLMææ¡ˆç”Ÿæˆä¸­â€¦")
        audits = {}
        total = len(pages)
        for i, (u, page) in enumerate(pages.items(), start=1):
            if st.session_state.cancel:
                break
            status_box.write(f"åˆ†æ {i}/{total}: {u}")
            try:
                audits[u] = page_audit(client, page, metrics_map[u])
            except Exception as e:
                audits[u] = {
                    "summary": "",
                    "top_issues": [f"LLMã‚¨ãƒ©ãƒ¼: {e.__class__.__name__}"],
                    "recommendations": []
                }

        progress.progress(1.0, text="å®Œäº†")
        return metrics_map, audits

    metrics_map, audits = run_async(main())
    st.session_state.running = False

    if not metrics_map:
        st.info("å¯¾è±¡ãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆè–„ã„ãƒšãƒ¼ã‚¸ã®ã¿/ä¸­æ–­ãªã©ï¼‰ã€‚")
        st.stop()

    # ===== é›†è¨ˆãƒ†ãƒ¼ãƒ–ãƒ« =====
    st.subheader("ãƒšãƒ¼ã‚¸åˆ¥ã‚¹ã‚³ã‚¢ï¼ˆSEO/UXï¼‰")
    rows = []
    for u, m in metrics_map.items():
        rows.append({
            "URL": u,
            "Title": (m.get("title") or "")[:60],
            "SEO": m.get("seo_score"),
            "UX": m.get("ux_score"),
            "Words": m.get("word_count"),
            "Alt%": m.get("images_alt_ratio"),
            "Links": m.get("internal_links"),
            "LD+JSON": "Yes" if m.get("has_ldjson") else "No",
            "Viewport": "Yes" if m.get("has_viewport") else "No",
            "MetaDesc": "Yes" if m.get("has_meta_description") else "No",
            "H1": "Yes" if m.get("has_h1") else "No",
        })
    st.dataframe(rows, use_container_width=True, hide_index=True)

    # ===== è©³ç´°ï¼ˆ1ãƒšãƒ¼ã‚¸ãšã¤ï¼‰=====
    st.subheader("è©³ç´°ï¼ˆææ¡ˆï¼‰")
    for u in sorted(metrics_map.keys()):
        with st.expander(u, expanded=False):
            m = metrics_map[u]
            a = audits.get(u, {})
            st.markdown(f"**Title:** {m.get('title','')}")
            st.markdown(f"- SEO: {m.get('seo_score')} / UX: {m.get('ux_score')}")
            st.markdown(f"- Words: {m.get('word_count')}  Links: {m.get('internal_links')}  Alt%: {m.get('images_alt_ratio')}")
            st.markdown(f"- LD+JSON: {'Yes' if m.get('has_ldjson') else 'No'} / Viewport: {'Yes' if m.get('has_viewport') else 'No'} / MetaDesc: {'Yes' if m.get('has_meta_description') else 'No'} / H1: {'Yes' if m.get('has_h1') else 'No'}")
            if a.get("summary"):
                st.markdown(f"**Summary:** {a['summary']}")
            if a.get("top_issues"):
                st.markdown("**Top Issues:**")
                for it in a["top_issues"]:
                    st.write(f"- {it}")
            if a.get("recommendations"):
                st.markdown("**Recommendations:**")
                for it in a["recommendations"]:
                    st.write(f"- {it}")

    # ===== ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ =====
    st.subheader("ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    # JSON
    bundle = {u: {"metrics": metrics_map[u], "audit": audits.get(u, {})} for u in metrics_map}
    buf = io.StringIO()
    json.dump(bundle, buf, ensure_ascii=False, indent=2)
    st.download_button("JSONï¼ˆå…¨ä»¶ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=buf.getvalue(),
                       file_name="audit_full.json", mime="application/json")

    # CSVï¼ˆã‚¹ã‚³ã‚¢ã‚µãƒãƒªï¼‰
    csv_buf = io.StringIO()
    fieldnames = ["URL","Title","SEO","UX","Words","Alt%","Links","LD+JSON","Viewport","MetaDesc","H1"]
    writer = csv.DictWriter(csv_buf, fieldnames=fieldnames)
    writer.writeheader()
    for r in rows:
        writer.writerow(r)
    st.download_button("CSVï¼ˆã‚¹ã‚³ã‚¢è¡¨ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_buf.getvalue(),
                       file_name="audit_scores.csv", mime="text/csv")
